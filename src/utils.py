import numpy as np
import matplotlib.pyplot as plt
import torch
import time
from tqdm import tqdm

def plot_images(images, title=None, figsize=(12, 4)):
    """
    Display a batch of images in a grid.
    
    Args:
        images: Tensor of images
        title: Optional title for the figure
        figsize: Figure size (width, height)
    """
    num_images = len(images)
    fig, axes = plt.subplots(1, num_images, figsize=figsize)
    if num_images == 1:
        axes = [axes]
    
    for i, image in enumerate(images):
        axes[i].imshow(image.squeeze().cpu(), cmap='gray')
        axes[i].axis('off')
    
    if title:
        plt.suptitle(title)
    plt.tight_layout()
    plt.show()

def visualize_forward_diffusion(diffusion_process, images, device, num_timesteps=5, steps=None):
    """
    Visualize the forward diffusion process on a batch of images.
    
    Args:
        diffusion_process: DiffusionProcess instance
        images: Tensor of images to visualize
        device: Device to run the diffusion on
        num_timesteps: Number of timesteps to show
        steps: Specific timesteps to visualize
    """
    if steps is None:
        timesteps = torch.linspace(0, diffusion_process.timesteps-1, num_timesteps).long()
    else:
        timesteps = torch.tensor(steps).long()
        num_timesteps = len(steps)

    fig, axes = plt.subplots(len(images), num_timesteps + 1, figsize=(2*(num_timesteps + 1), 2*len(images)))    
    
    for i, image in enumerate(images):
        # Show original image
        axes[i, 0].imshow(image.squeeze().cpu(), cmap='gray')
        axes[i, 0].set_title('Original')
        axes[i, 0].axis('off')

        # Show noisy versions
        for j, t in enumerate(timesteps, 1):
            # Move everything to the same device
            img = image.unsqueeze(0).to(device)
            t_tensor = torch.tensor([t], device=device)
            noisy_image, _ = diffusion_process.corrupt_image(img, t_tensor)
            axes[i, j].imshow(noisy_image.squeeze().cpu(), cmap='gray')
            axes[i, j].set_title(f't={t.item()}')
            axes[i, j].axis('off')

    plt.tight_layout()
    plt.show()

def run_sampling_experiment(diffusion_model, model, n_samples=4, img_size=(1, 32, 32), 
                          device='cpu', timesteps=1000, steps_list=[100, 50, 20, 10]):
    """
    Compare DDPM and DDIM sampling with different numbers of steps.
    
    Args:
        diffusion_model: The DDIM instance
        model: The trained noise prediction model
        n_samples: Number of images to generate
        img_size: Size of each image
        device: Device to run the sampling on
        timesteps: Total number of timesteps
        steps_list: List of different step counts to try for DDIM
    
    Returns:
        Dictionary with sampling results
    """
    # Dictionary to store results
    results = {}
    
    # DDPM with all steps
    start_time = time.time()
    ddpm_samples = diffusion_model.sample(model, n_samples, device, img_size)
    ddpm_time = time.time() - start_time
    results["DDPM (all steps)"] = {"time": ddpm_time, "samples": ddpm_samples}
    print(f"DDPM ({timesteps} steps) sampling time: {ddpm_time:.2f}s")
    
    # DDIM with different numbers of steps
    for steps in steps_list:
        # Create a subset of timesteps
        skip = timesteps // steps
        timestep_subset = list(range(0, timesteps, skip))
        if timestep_subset[-1] != timesteps - 1:
            timestep_subset.append(timesteps - 1)
            
        # DDIM with eta=0 (deterministic)
        start_time = time.time()
        ddim_samples = diffusion_model.sample_ddim(model, n_samples, device, img_size, 
                                               timestep_subset=timestep_subset, eta=0.0)
        ddim_time = time.time() - start_time
        results[f"DDIM (η=0, {len(timestep_subset)} steps)"] = {"time": ddim_time, "samples": ddim_samples}
        print(f"DDIM (η=0, {len(timestep_subset)} steps) sampling time: {ddim_time:.2f}s")
        
        # DDIM with eta=1 (stochastic like DDPM)
        start_time = time.time()
        ddim_stochastic_samples = diffusion_model.sample_ddim(model, n_samples, device, img_size, 
                                                         timestep_subset=timestep_subset, eta=1.0)
        ddim_stochastic_time = time.time() - start_time
        results[f"DDIM (η=1, {len(timestep_subset)} steps)"] = {"time": ddim_stochastic_time, "samples": ddim_stochastic_samples}
        print(f"DDIM (η=1, {len(timestep_subset)} steps) sampling time: {ddim_stochastic_time:.2f}s")
    
    return results

def visualize_sampling_results(results):
    """
    Visualize samples generated by different methods and their timing.
    
    Args:
        results: Dictionary with sampling results
    """
    # Display generated samples
    fig, axes = plt.subplots(len(results), 4, figsize=(16, 3*len(results)))
    
    for i, (method, result) in enumerate(results.items()):
        samples = result["samples"]
        time_taken = result["time"]
        
        for j in range(4):
            axes[i, j].imshow(samples[j].squeeze().cpu(), cmap='gray')
            axes[i, j].set_title(f"Sample {j+1}")
            axes[i, j].axis('off')
        
        # Method name and time
        axes[i, 0].set_ylabel(f"{method}\nTime: {time_taken:.2f}s", fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    # Create a bar chart comparing sampling times
    methods = list(results.keys())
    times = [result["time"] for result in results.values()]
    
    plt.figure(figsize=(12, 6))
    bars = plt.bar(methods, times)
    plt.ylabel('Sampling Time (seconds)')
    plt.title('Comparison of Sampling Time: DDPM vs DDIM')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Add time values on top of the bars
    for bar, time_val in zip(bars, times):
        plt.text(bar.get_x() + bar.get_width()/2, time_val + 0.02*max(times), 
                 f"{time_val:.2f}s", ha='center', va='bottom')
    
    plt.show()

def encode_decode_experiment(diffusion_model, model, images, device, steps_subset=None, timesteps=1000):
    """
    Encode real images to latent space and decode them back.
    
    Args:
        diffusion_model: The DDIM instance
        model: The trained noise prediction model
        images: Tensor of images to encode
        device: Device to run on
        steps_subset: Number of steps to use (if None, use all timesteps)
        timesteps: Total number of timesteps
    
    Returns:
        tuple: (encoded_latents, decoded_images)
    """
    n_images = len(images)
    
    # Define subset of timesteps if provided
    if steps_subset is not None:
        skip = timesteps // steps_subset
        timestep_subset = list(range(0, timesteps, skip))
        if timestep_subset[-1] != timesteps - 1:
            timestep_subset.append(timesteps - 1)
    else:
        timestep_subset = None
    
    # Encoding phase
    print("Encoding real images to latent space...")
    encoded_latents = diffusion_model.encode_ddim(model, images, device, timestep_subset=timestep_subset)
    
    # Show the latents (they should look like noise)
    plt.figure(figsize=(12, 3))
    for i in range(n_images):
        plt.subplot(1, n_images, i+1)
        plt.imshow(encoded_latents[i].squeeze().cpu(), cmap='gray')
        plt.title(f"Latent {i+1}")
        plt.axis('off')
    plt.tight_layout()
    plt.show()
    
    # Decoding phase
    print("Decoding latents back to images...")
    if steps_subset is not None:
        decoded_images = diffusion_model.sample_ddim(
            model, n_images, device, size=images[0].shape, 
            timestep_subset=list(reversed(timestep_subset)), eta=0.0,
            x_T=encoded_latents  # Use the encoded latents as the starting point
        )
    else:
        decoded_images = diffusion_model.sample_ddim(
            model, n_images, device, size=images[0].shape, eta=0.0,
            x_T=encoded_latents  # Use the encoded latents as the starting point
        )
    
    # Display the results
    fig, axes = plt.subplots(n_images, 2, figsize=(8, 2*n_images))
    for i in range(n_images):
        # Original image
        axes[i, 0].imshow(images[i].squeeze().cpu(), cmap='gray')
        axes[i, 0].set_title(f"Original {i+1}")
        axes[i, 0].axis('off')
        
        # Reconstructed image
        axes[i, 1].imshow(decoded_images[i].squeeze().cpu(), cmap='gray')
        axes[i, 1].set_title(f"Reconstructed {i+1}")
        axes[i, 1].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return encoded_latents, decoded_images

def visualize_denoising_process(diffusion_model, model, device, n_samples=2, steps_to_show=10, sampling_steps=100, img_size=(1, 32, 32)):
    """
    Visualize the denoising process for both DDPM (DDIM with η=1) and deterministic DDIM (η=0).
    
    Args:
        diffusion_model: The DDIM instance
        model: The trained noise prediction model
        device: Device to run on
        n_samples: Number of images to generate
        steps_to_show: Number of intermediate steps to visualize in the plot (last k steps)
        sampling_steps: Number of steps to use for sampling (controls sampling speed)
        img_size: Size of generated images (channels, height, width)
    """
    model.eval()
    with torch.no_grad():
        # Generate the same starting noise for both methods
        x_T = torch.randn((n_samples, *img_size)).to(device)
        
        # Create a subset of timesteps for sampling
        timesteps = diffusion_model.timesteps
        step_size = timesteps // sampling_steps
        timestep_subset = list(range(timesteps - 1, -1, -step_size))
        # Ensure the last step (t=0) is included
        if timestep_subset[-1] != 0:
            timestep_subset.append(0)
        
        # Select the last k steps for visualization (from the sampling process)
        steps_to_show = min(steps_to_show, len(timestep_subset))
        viz_indices = list(range(len(timestep_subset) - steps_to_show, len(timestep_subset)))
        
        # For tracking intermediate results
        ddpm_intermediates = []
        ddim_intermediates = []
        
        print("Generating DDPM (DDIM with η=1) images...")
        # Use DDIM with η=1 for DDPM-like sampling
        x_ddpm, intermediates_ddpm = diffusion_model.sample_ddim_with_intermediates(
            model=model,
            n_samples=n_samples,
            device=device,
            size=img_size,
            timestep_subset=timestep_subset,
            eta=1.0,  # Set η=1 for DDPM-equivalent behavior
            x_T=x_T,
            save_intermediates_at=viz_indices
        )
        ddpm_intermediates = intermediates_ddpm
        
        print("Generating deterministic DDIM (η=0) images...")
        # Use DDIM with η=0 for deterministic sampling
        x_ddim, intermediates_ddim = diffusion_model.sample_ddim_with_intermediates(
            model=model,
            n_samples=n_samples,
            device=device,
            size=img_size,
            timestep_subset=timestep_subset,
            eta=0.0,  # Set η=0 for deterministic DDIM
            x_T=x_T,
            save_intermediates_at=viz_indices
        )
        ddim_intermediates = intermediates_ddim
    
    model.train()
    
    # Extract the actual timestep values for the visualized steps
    visualization_steps = [timestep_subset[idx] for idx in viz_indices]
    
    # Plot the intermediate images
    fig, axes = plt.subplots(2 * n_samples, steps_to_show, figsize=(18, 4 * n_samples))
    
    # Add row labels outside the plots for better visibility
    row_labels = ['DDPM (η=1)', 'DDIM (η=0)'] * n_samples
    for i in range(2 * n_samples):
        sample_num = (i // 2) + 1
        row_label = f"{row_labels[i]}\nSample {sample_num}"
        
        # Position the row label
        if steps_to_show > 1:
            fig.text(0.01, 1 - (i + 0.5) / (2 * n_samples), row_label, 
                     va='center', ha='left', fontsize=14, 
                     bbox=dict(facecolor='white', alpha=0.8, edgecolor='lightgray'))
    
    for sample_idx in range(n_samples):
        # DDPM process (DDIM with η=1)
        for i in range(steps_to_show):
            if n_samples > 1 or steps_to_show > 1:
                ax = axes[2*sample_idx, i]
            else:
                ax = axes[2*sample_idx]
                
            img = ddpm_intermediates[i][sample_idx]
            ax.imshow(img.squeeze().cpu(), cmap='gray')
            t_val = visualization_steps[i]
            ax.set_title(f"t={t_val}")
            ax.axis('off')
        
        # DDIM process (η=0)
        for i in range(steps_to_show):
            if n_samples > 1 or steps_to_show > 1:
                ax = axes[2*sample_idx+1, i]
            else:
                ax = axes[2*sample_idx+1]
                
            img = ddim_intermediates[i][sample_idx]
            ax.imshow(img.squeeze().cpu(), cmap='gray')
            t_val = visualization_steps[i]
            ax.set_title(f"t={t_val}")
            ax.axis('off')
    
    plt.suptitle(f'Comparison of Denoising Process: DDPM vs DDIM with {sampling_steps} steps\nShowing last {steps_to_show} sampling steps', fontsize=16)
    plt.tight_layout()
    plt.subplots_adjust(top=0.9, left=0.08)
    plt.show()
