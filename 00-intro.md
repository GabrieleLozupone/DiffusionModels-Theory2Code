
<style>
    .row {
        display: flex;
        flex-direction: row;
        align-items: flex-start;
        gap: 40px;
        margin-bottom: 1em;
    }
    .column {
        flex: 1;
    }
</style>
<div>

# ğŸ§  0. Introduction & Motivation

Generative models are a cornerstone of modern machine learning, enabling systems that can *create entirely new data*â€”images, audio, textâ€”by learning from real-world distributions.

## ğŸ¤– What Are Generative Models?

Three major paradigms have dominated the space:

<div class="row">
  <div class="column">
    <ul>
      <li><b>Generative Adversarial Networks (GANs)</b><br>
        Introduced by Goodfellow et al. [1], GANs use a game between a generator and a discriminator.<br>
        âœ… High-quality images<br>
        âŒ Training instability, no likelihood estimation
      </li>
    </ul>
  </div>
  <div class="column">
    <img src="images/gan.png" alt="GAN Illustration" width="100%">
  </div>
</div>

<div class="row">
  <div class="column">
    <ul>
      <li><b>Variational Autoencoders (VAEs)</b><br>
        Proposed by Kingma and Welling [2], VAEs combine deep learning with probabilistic modeling.<br>
        âœ… Principled latent space, tractable likelihood<br>
        âŒ Blurry samples due to Gaussian assumptions
      </li>
    </ul>
  </div>
  <div class="column">
    <img src="images/vae-2.png" alt="VAE Illustration" width="100%">
  </div>
</div>

---

## ğŸŒ€ Why Diffusion Models?

Diffusion models offer a compelling alternative with unique advantages:

- **Stability**: No adversarial training  
- **Likelihood-based**: Optimized using a well-defined ELBO objective  
- **Sample quality**: Comparable or superior to GANs  
- **Scalability**: Parallelizable training and flexible conditioning

Originally proposed in Sohl-Dickstein et al. [3], and later improved as Denoising Diffusion Probabilistic Models (DDPM) by Ho et al. [4], diffusion models learn to denoise a sample iteratively from pure noiseâ€”essentially learning a reverse stochastic process.

---

## ğŸ“ˆ Real-World Applications

| Domain             | Use Case                                          |
|--------------------|---------------------------------------------------|
| Image generation   | Stable Diffusion, Imagen, DALLE-2 [5][6][7]       |
| Inpainting/editing | Photoshop AI fill, restoration tools              |
| Molecular design   | Protein folding, molecule generation [8]          |
| Medical imaging    | MRI super-resolution, anomaly detection [9][10]   |

These models are reshaping not just AI research, but entire industries.

---

## ğŸ“š References

1. Goodfellow et al., â€œGenerative Adversarial Networks,â€ NeurIPS 2014.  
2. Kingma & Welling, â€œAuto-Encoding Variational Bayes,â€ ICLR 2014.  
3. Sohl-Dickstein et al., â€œDeep Unsupervised Learning using Nonequilibrium Thermodynamics,â€ ICML 2015.  
4. Ho et al., â€œDenoising Diffusion Probabilistic Models,â€ NeurIPS 2020.  
5. Rombach et al., â€œHigh-Resolution Image Synthesis with Latent Diffusion Models,â€ CVPR 2022.  
6. Saharia et al., â€œImagen: Photorealistic Text-to-Image Generation,â€ ICML 2022.  
7. Ramesh et al., â€œHierarchical Text-Conditional Image Generation with CLIP Latents,â€ arXiv 2022.  
8. Hoogeboom et al., â€œEquivariant Diffusion for Molecule Generation in 3D,â€ ICML 2022.  
9. Wolleb et al., â€œDiffusion Models for Medical Anomaly Detection,â€ MICCAI 2022.  
10. Pinaya et al., â€œBrain Imaging Generation with Latent Diffusion Models,â€ NeuroImage 2022.
<div>
</div>
</div>